{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1NGQk5DeWO3U/oLSaB+qI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Exar2004/fatec_PLN_Codes/blob/main/C%C3%B3pia_de_Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvZWWIwB8Fyq",
        "outputId": "3ebef6bd-ce6f-4b20-bfc0-6437600d76c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Olá!!! Este é um exemplo de texto, com várias PONTUAÇÕES, símbolos #especiais, e LETRAS maiúsculas.\n",
            "\n",
            "Texto limpo: Olá Este é um exemplo de texto com várias PONTUAÇÕES símbolos especiais e LETRAS maiúsculas\n",
            "\n",
            "Texto normalizado: olá este é um exemplo de texto com várias pontuações símbolos especiais e letras maiúsculas\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "original = \"Olá!!! Este é um exemplo de texto, com várias PONTUAÇÕES, símbolos #especiais, e LETRAS maiúsculas.\"\n",
        "\n",
        "texto_limpo = re.sub(r'[^A-Za-zA-ŷ\\s]', '', original)\n",
        "\n",
        "texto_normalizado = texto_limpo.lower()\n",
        "\n",
        "print(f'Texto original: {original}')\n",
        "print(f'\\nTexto limpo: {texto_limpo}')\n",
        "print(f'\\nTexto normalizado: {texto_normalizado}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(texto_normalizado)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8QamTzn9vwY",
        "outputId": "bef0a919-e6cb-4c5d-be42-d80cddd3b54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['olá', 'este', 'é', 'um', 'exemplo', 'de', 'texto', 'com', 'várias', 'pontuações', 'símbolos', 'especiais', 'e', 'letras', 'maiúsculas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "tokens_sem_stopwords = [palavra for palavra in tokens if palavra.lower() not in stopwords_pt]\n",
        "print(tokens_sem_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDLXGbkoQUJ-",
        "outputId": "05187e90-5733-4f5c-d4cb-c5960ac42f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['olá', 'exemplo', 'texto', 'várias', 'pontuações', 'símbolos', 'especiais', 'letras', 'maiúsculas']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "nltk.download('rslp')\n",
        "\n",
        "stemmer = RSLPStemmer()\n",
        "stemming = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]\n",
        "print(stemming)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5Gsx-e6WOrv",
        "outputId": "30b07f9c-d0cb-423f-bd77-f090057cbd72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['olá', 'exempl', 'text', 'vár', 'pontu', 'símbol', 'espec', 'letr', 'maiúscul']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')\n",
        "#Texto de exemplo.\n",
        "texto = input(\"Insira um texto que seja coerente, podendo ter simbolos: \")\n",
        "#Limpeza de ruidos e normalização\n",
        "texto_limpo = re.sub(r'[^a-zA-Z]','', texto) # Remove tudo que não for letra e substitui por espaço\n",
        "texto_lower = texto_limpo.lower() # Converte para minúsculas\n",
        "#Tokenização\n",
        "tokens = nltk.word_tokenize(texto_lower)\n",
        "#Remoção de stopwords\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "palavras_filtradas = [palavra for palavra in tokens if palavra not in stop_words]\n",
        "#Stemming\n",
        "stemmer = PorterStemmer()\n",
        "palavras_stemizadas = [stemmer.stem(palavra) for palavra in palavras_filtradas]\n",
        "#Impressão do resultado final\n",
        "print(palavras_stemizadas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDzoUkl7YfDW",
        "outputId": "980a8f03-e722-4167-cc94-07b1ecc0d119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insira um texto que seja coerente, podendo ter simbolos: porfavor codigo mal feito funciona!!!!!\n",
            "['porfavorcodigomalfeitofunciona']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "documentos = [\n",
        "\"gato e cachorro\",\n",
        "\"gato brinca com cachorro\",\n",
        "\"gato e rato\"\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documentos)\n",
        "print(\"Matriz BoW:\")\n",
        "print(X.toarray())\n",
        "print(f\"Vocabulario: {vectorizer.vocabulary_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHoiml9paUvL",
        "outputId": "628a980c-6cf9-4386-d19e-b296d10f77d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz BoW:\n",
            "[[0 1 0 1 0]\n",
            " [1 1 1 1 0]\n",
            " [0 0 0 1 1]]\n",
            "Vocabulario: {'gato': 3, 'cachorro': 1, 'brinca': 0, 'com': 2, 'rato': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "documentos = [\n",
        "\"O cachorro gosta de passear no parque\",\n",
        "\"O gato dorme no sofá o dia todo\",\n",
        "\"Cachorros e gatos podem ser bons amigos\"\n",
        "]\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_bow = vectorizer_bow.fit_transform(documentos)\n",
        "print(f\"Vocabulário Bow: {vectorizer_bow.vocabulary_}\")\n",
        "print(\"Matriz Bow:\")\n",
        "print(X_bow.toarray())\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(documentos)\n",
        "print(f\"\\nVocabulário TF-IDF: (vectorizer_tfidf.vocabulary_)\")\n",
        "print(\"Matriz TF-IDF\")\n",
        "print(X_tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIXI64SFiVVj",
        "outputId": "de8e3b40-2273-460b-8712-6a8ccaad2ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulário Bow: {'cachorro': 2, 'gosta': 9, 'de': 4, 'passear': 12, 'no': 10, 'parque': 11, 'gato': 7, 'dorme': 6, 'sofá': 15, 'dia': 5, 'todo': 16, 'cachorros': 3, 'gatos': 8, 'podem': 13, 'ser': 14, 'bons': 1, 'amigos': 0}\n",
            "Matriz Bow:\n",
            "[[0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0]\n",
            " [0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1]\n",
            " [1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0]]\n",
            "\n",
            "Vocabulário TF-IDF: (vectorizer_tfidf.vocabulary_)\n",
            "Matriz TF-IDF\n",
            "[[0.         0.         0.42339448 0.         0.42339448 0.\n",
            "  0.         0.         0.         0.42339448 0.32200242 0.42339448\n",
            "  0.42339448 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.42339448\n",
            "  0.42339448 0.42339448 0.         0.         0.32200242 0.\n",
            "  0.         0.         0.         0.42339448 0.42339448]\n",
            " [0.40824829 0.40824829 0.         0.40824829 0.         0.\n",
            "  0.         0.         0.40824829 0.         0.         0.\n",
            "  0.         0.40824829 0.40824829 0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Import nitk\n",
        "import re\n",
        "from altk.corpus isport stopwords\n",
        "from nitk.tokenize import word tokenize\n",
        "from nitk.stem import WordlietLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "oltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nitk.download(\"wordnet\")\n",
        "nitk.download(\"one-1.4\")\n",
        "documentos1[\n",
        "\"Os cachorros são animais muito amigáveis e leaisi\",\n",
        "1\n",
        "\"Eu gusto de gatos porque eles são Independentes e fufus.\", \"Cachorros gatos poden ser ótimos animais de estimaçãoς,\"\n",
        "]\n",
        "lemmatizer = MordletLemmatizer()\n",
        "stop_words = set(stupwords.words(\"portuguese\"))\n",
        "def preprocessar_texto(texto):\n",
        "texto = re.sub(r'[^a-zA-24-\\s].\",texto)\n",
        "tokens_tudo = word_tokenize(texto.lower())\n",
        "tokens =  (palavra for palavra in tokens tudo if palavra not in stop words tokens lema (lemmatizer, lemmatico (palavra) for palavra in tokens\n",
        "return''.join(tokens lewa)\n",
        "documentos processados preprocessar texto doc) for doc in documentus)\n",
        "print(\"documentos Pre-processados:\")\n",
        "for i, doc in enumerate(documentos processados):\n",
        "print (f\"Documento (11) (doc)\").\n",
        "vectorizer CountVectorizer()\n",
        "Xhow vectorizar.fit transform(documentos_processados)\n",
        "print(\"\\nVocabulário Bow:\", vectorizer.vocabulary)\n",
        "print(\"Matria B:\")\n",
        "print(X bow.toarray())"
      ],
      "metadata": {
        "id": "8-GxVhTJoVJ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
